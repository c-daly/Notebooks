{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nimport os\nfrom tensorflow.keras.layers import Activation, Dense, Conv2D, BatchNormalization, LeakyReLU, Reshape, Conv2DTranspose, Input, UpSampling2D\nimport time\nimport matplotlib.pyplot as plt\n\nfrom IPython import display\n\n#tf.config.run_functions_eagerly(True)\n\nBATCH_SIZE = 16","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-14T15:34:40.881303Z","iopub.execute_input":"2023-07-14T15:34:40.882266Z","iopub.status.idle":"2023-07-14T15:34:50.291197Z","shell.execute_reply.started":"2023-07-14T15:34:40.882225Z","shell.execute_reply":"2023-07-14T15:34:50.289924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_name = '/kaggle/input/gan-getting-started/monet_jpg'\n#os.makedirs(\"/saved_images\")\nimages_dataset = tf.keras.utils.image_dataset_from_directory(dir_name, batch_size=None, labels=None)\nimages_dataset = images_dataset.map(lambda image: tf.image.resize(image, (256, 256)) / 255.0)\nprint(images_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:50.294156Z","iopub.execute_input":"2023-07-14T15:34:50.294873Z","iopub.status.idle":"2023-07-14T15:34:50.629810Z","shell.execute_reply.started":"2023-07-14T15:34:50.294838Z","shell.execute_reply":"2023-07-14T15:34:50.628448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = images_dataset.shuffle(buffer_size=300).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:50.631477Z","iopub.execute_input":"2023-07-14T15:34:50.631913Z","iopub.status.idle":"2023-07-14T15:34:50.641470Z","shell.execute_reply.started":"2023-07-14T15:34:50.631883Z","shell.execute_reply":"2023-07-14T15:34:50.640175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_dc_generator():\n    model = tf.keras.Sequential()\n\n    model.add(layers.Dense(16*16*128, use_bias=False, input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((16, 16, 128)))\n\n    model.add(layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    #model.add(layers.UpSampling2D())\n    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n\n    #assert model.output_shape == (None, 256, 256, 3)\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:50.645280Z","iopub.execute_input":"2023-07-14T15:34:50.645768Z","iopub.status.idle":"2023-07-14T15:34:50.658782Z","shell.execute_reply.started":"2023-07-14T15:34:50.645726Z","shell.execute_reply":"2023-07-14T15:34:50.657819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generator = make_generator_model()\ngenerator = build_dc_generator()\nnoise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\ngenerated_image = (generated_image + 1) / 2.0\ngenerated_image = np.clip(generated_image.numpy().squeeze(), 0, 1)\n#plt.imshow(generated_image[0, :, :, 3], cmap='grey')\nplt.imshow(generated_image)\nplt.show()\ngenerated_image = np.expand_dims(generated_image, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:50.660071Z","iopub.execute_input":"2023-07-14T15:34:50.661108Z","iopub.status.idle":"2023-07-14T15:34:51.421171Z","shell.execute_reply.started":"2023-07-14T15:34:50.661067Z","shell.execute_reply":"2023-07-14T15:34:51.419983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n\n    model.add(layers.Conv2D(16, (3, 3), strides=(2, 2), padding='same',\n                                     input_shape=[256, 256, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:51.422765Z","iopub.execute_input":"2023-07-14T15:34:51.423959Z","iopub.status.idle":"2023-07-14T15:34:51.435405Z","shell.execute_reply.started":"2023-07-14T15:34:51.423917Z","shell.execute_reply":"2023-07-14T15:34:51.434490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator = make_discriminator_model()\ndecision = discriminator(generated_image)\nprint (decision)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:51.436774Z","iopub.execute_input":"2023-07-14T15:34:51.437109Z","iopub.status.idle":"2023-07-14T15:34:51.620556Z","shell.execute_reply.started":"2023-07-14T15:34:51.437081Z","shell.execute_reply":"2023-07-14T15:34:51.619405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:51.621875Z","iopub.execute_input":"2023-07-14T15:34:51.622184Z","iopub.status.idle":"2023-07-14T15:34:51.631100Z","shell.execute_reply.started":"2023-07-14T15:34:51.622158Z","shell.execute_reply":"2023-07-14T15:34:51.630028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:51.632227Z","iopub.execute_input":"2023-07-14T15:34:51.632590Z","iopub.status.idle":"2023-07-14T15:34:51.638899Z","shell.execute_reply.started":"2023-07-14T15:34:51.632549Z","shell.execute_reply":"2023-07-14T15:34:51.637770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:51.643300Z","iopub.execute_input":"2023-07-14T15:34:51.643759Z","iopub.status.idle":"2023-07-14T15:34:51.650184Z","shell.execute_reply.started":"2023-07-14T15:34:51.643726Z","shell.execute_reply":"2023-07-14T15:34:51.649240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:51.651486Z","iopub.execute_input":"2023-07-14T15:34:51.651921Z","iopub.status.idle":"2023-07-14T15:34:51.667342Z","shell.execute_reply.started":"2023-07-14T15:34:51.651894Z","shell.execute_reply":"2023-07-14T15:34:51.666117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n    gen_loss_sum = 0\n    disc_loss_sum = 0\n    num_batches = 0\n\n    for image_batch in dataset:\n      gen_loss, disc_loss = train_step(image_batch)\n      gen_loss_sum += gen_loss\n      disc_loss_sum += disc_loss\n      num_batches += 1\n\n    gen_loss_avg = gen_loss_sum / num_batches\n    disc_loss_avg = disc_loss_sum / num_batches\n\n    # Produce images for the GIF as you go\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n    # Save the model every 15 epochs\n    #if (epoch + 1) % 100 == 0:\n    #  checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n    print(f\"Loss:\\n\")\n    print(f\"Epoch {epoch + 1}, Gen loss: {gen_loss_avg}, Disc loss: {disc_loss_avg}\")\n  # Generate after the final epoch\n  display.clear_output(wait=True)\n  generate_and_save_images(generator,\n                           epochs,\n                           seed)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:51.669031Z","iopub.execute_input":"2023-07-14T15:34:51.669861Z","iopub.status.idle":"2023-07-14T15:34:51.681638Z","shell.execute_reply.started":"2023-07-14T15:34:51.669817Z","shell.execute_reply":"2023-07-14T15:34:51.680634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10000\nnoise_dim = 100\nnum_examples_to_generate = 16\n\n# You will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:51.682968Z","iopub.execute_input":"2023-07-14T15:34:51.683314Z","iopub.status.idle":"2023-07-14T15:34:51.701209Z","shell.execute_reply.started":"2023-07-14T15:34:51.683285Z","shell.execute_reply":"2023-07-14T15:34:51.700016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    gen_loss = None\n    disc_loss = None\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n\n    return gen_loss, disc_loss","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:51.703055Z","iopub.execute_input":"2023-07-14T15:34:51.703434Z","iopub.status.idle":"2023-07-14T15:34:51.715633Z","shell.execute_reply.started":"2023-07-14T15:34:51.703402Z","shell.execute_reply":"2023-07-14T15:34:51.714517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_and_save_images(model, epoch, test_input):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n  predictions = model(test_input, training=False)\n\n  fig = plt.figure(figsize=(4, 4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.axis('off')\n      plt.imshow(predictions[i])\n\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:51.716856Z","iopub.execute_input":"2023-07-14T15:34:51.718016Z","iopub.status.idle":"2023-07-14T15:34:51.734107Z","shell.execute_reply.started":"2023-07-14T15:34:51.717977Z","shell.execute_reply":"2023-07-14T15:34:51.733073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.shape(train_dataset)\ntrain(train_dataset, EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:34:51.735805Z","iopub.execute_input":"2023-07-14T15:34:51.736232Z","iopub.status.idle":"2023-07-14T19:50:50.808143Z","shell.execute_reply.started":"2023-07-14T15:34:51.736192Z","shell.execute_reply":"2023-07-14T19:50:50.806473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:50:57.606160Z","iopub.execute_input":"2023-07-14T19:50:57.606992Z","iopub.status.idle":"2023-07-14T19:50:57.644091Z","shell.execute_reply.started":"2023-07-14T19:50:57.606955Z","shell.execute_reply":"2023-07-14T19:50:57.642914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/saved_images\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:04:31.644553Z","iopub.execute_input":"2023-07-14T20:04:31.644967Z","iopub.status.idle":"2023-07-14T20:04:31.803524Z","shell.execute_reply.started":"2023-07-14T20:04:31.644938Z","shell.execute_reply":"2023-07-14T20:04:31.801903Z"},"trusted":true},"execution_count":21,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/saved_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n","\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/kaggle/working/saved_images'"],"ename":"FileExistsError","evalue":"[Errno 17] File exists: '/kaggle/working/saved_images'","output_type":"error"}]},{"cell_type":"code","source":"import shutil\n\nfor x in range(7000):\n    if x % 1000 == 0:\n        print(x)\n    noise = tf.random.normal([1, 100])\n    generated_image = generator(noise, training=False)\n    \n    filename = f\"/kaggle/working/saved_images/{x}.jpg\"\n    \n    tf.keras.utils.save_img(filename, generated_image[0], file_format=\"jpeg\")\n    \n\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/saved_images\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:04:36.233107Z","iopub.execute_input":"2023-07-14T20:04:36.233489Z","iopub.status.idle":"2023-07-14T20:08:22.381962Z","shell.execute_reply.started":"2023-07-14T20:04:36.233460Z","shell.execute_reply":"2023-07-14T20:08:22.380823Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"0\n1000\n2000\n3000\n4000\n5000\n6000\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/images.zip'"},"metadata":{}}]}]}